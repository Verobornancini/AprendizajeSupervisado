{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats \n",
    "import seaborn as sns \n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "#from sklearn.neighbors import KNeighborsRegressor\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.ensemble import ExtraTreesRegressor\n",
    "#from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from IPython.display import display_html\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.ensemble import BalancedBaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el dataset original\n",
    "original_df = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(train_data_fname, test_data_fname):\n",
    "    \n",
    "    df_train = pd.read_csv(train_data_fname)\n",
    "    df_train['is_train_set'] = 1\n",
    "    \n",
    "    df_test = pd.read_csv(test_data_fname)\n",
    "    df_test['is_train_set'] = 0\n",
    "    \n",
    "    df_train.fillna({'Upc': original_df.Upc.mode()[0]}, inplace=True)\n",
    "    df_test.fillna({'Upc': original_df.Upc.mode()[0]}, inplace=True)\n",
    "    df_train['ItemCode'] = df_train['Upc'].apply(\n",
    "      lambda x: str(x)[6:10])\n",
    "    df_test['ItemCode'] = df_test['Upc'].apply(\n",
    "      lambda x: str(x)[6:10]\n",
    "    )\n",
    "    # we  get the TripType for the train set. To do that, we group by VisitNumber and\n",
    "    # then we get the max (or min or avg)\n",
    "    y = df_train.groupby([\"VisitNumber\", \"Weekday\", \"ItemCode\"], as_index=False).max().TripType\n",
    "    # we remove the TripType now, and concat training and testing data\n",
    "    # the concat is done so that we have the same columns for both datasets\n",
    "    # after one-hot encoding\n",
    "    df_train = df_train.drop(\"TripType\", axis=1)\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    \n",
    "    # the next three operations are the ones we have just presented in the previous lines\n",
    "    \n",
    "    # drop the columns we won't use (it may be good to use them somehow)\n",
    "    df_notna = df.drop([\"FinelineNumber\"], axis=1)\n",
    "    \n",
    "    # one-hot encoding for the DepartmentDescription\n",
    "    df_notna = pd.get_dummies(df_notna, columns=[\"DepartmentDescription\"], dummy_na=True)\n",
    "    # now we add the groupby values\n",
    "    df_notna = df_notna.groupby([\"VisitNumber\", \"Weekday\", \"ItemCode\"], as_index=False).sum()\n",
    "    \n",
    "    # finally, we do one-hot encoding for the Weekday\n",
    "    df_notna = pd.get_dummies(df_notna, columns=[\"Weekday\"], dummy_na=True)\n",
    "    # one-hot encoding for the ItemCode\n",
    "    df_notna = pd.get_dummies(df_notna, columns=[\"ItemCode\"], dummy_na=True)\n",
    "    # get train and test back\n",
    "    df_train = df_notna[df_notna.is_train_set != 0]\n",
    "    df_test = df_notna[df_notna.is_train_set == 0]\n",
    "    \n",
    "    X = df_train.drop([\"is_train_set\"], axis=1)\n",
    "    yy = None\n",
    "    XX = df_test.drop([\"is_train_set\"], axis=1)\n",
    "    return X, y, XX, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la información del dataset\n",
    "X, y, XX, yy = transform_data(\"../data/train.csv\", \"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((423935, 10420), (423935,), (180832, 10420))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape, XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
